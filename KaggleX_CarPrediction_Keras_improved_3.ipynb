{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSlpgOUrZtplVUZfk4ZhKY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGhvwHRZcO9Z",
        "outputId": "34100c0a-3fee-4ebc-915f-13d119acebf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.1)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.3\n",
            "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 122325135.84253745\n",
            "R2 Score: 0.666120826462095\n"
          ]
        }
      ],
      "source": [
        "!pip install category_encoders\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import category_encoders as ce\n",
        "import numpy as np\n",
        "import re\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import os\n",
        "\n",
        "# Load the data\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "\n",
        "# Function to limit categories to the top 5 most common\n",
        "def limit_categories(df, column):\n",
        "    top_5 = df[column].value_counts().index[:5]\n",
        "    df[column] = df[column].apply(lambda x: x if x in top_5 else 'Other')\n",
        "    return df\n",
        "\n",
        "# Apply the function to both int_col and ext_col in train and test datasets\n",
        "train_df = limit_categories(train_df, 'int_col')\n",
        "train_df = limit_categories(train_df, 'ext_col')\n",
        "test_df = limit_categories(test_df, 'int_col')\n",
        "test_df = limit_categories(test_df, 'ext_col')\n",
        "\n",
        "# Extract numerical values from the engine column\n",
        "def extract_engine_power(engine_str):\n",
        "    match = re.search(r'(\\d+(\\.\\d+)?)HP', engine_str)\n",
        "    if match:\n",
        "        return float(match.group(1))\n",
        "    return 0\n",
        "\n",
        "train_df['engine_power'] = train_df['engine'].apply(extract_engine_power)\n",
        "test_df['engine_power'] = test_df['engine'].apply(extract_engine_power)\n",
        "\n",
        "# Drop the original engine column\n",
        "train_df.drop(columns=['engine'], inplace=True)\n",
        "test_df.drop(columns=['engine'], inplace=True)\n",
        "\n",
        "# Add age of the car\n",
        "train_df['age'] = 2024 - train_df['model_year']\n",
        "test_df['age'] = 2024 - test_df['model_year']\n",
        "\n",
        "# Remove outliers based on the price column\n",
        "q1 = train_df['price'].quantile(0.25)\n",
        "q3 = train_df['price'].quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "train_df = train_df[(train_df['price'] >= lower_bound) & (train_df['price'] <= upper_bound)]\n",
        "\n",
        "# Separate the target column from the training set\n",
        "y = train_df['price']\n",
        "X_train_full = train_df.drop(columns=['price'])\n",
        "\n",
        "# Encoding categorical variables\n",
        "categorical_features = ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title']\n",
        "encoder = ce.OneHotEncoder(cols=categorical_features, handle_unknown='ignore', use_cat_names=True)\n",
        "X_train_encoded = encoder.fit_transform(X_train_full)\n",
        "X_test_encoded = encoder.transform(test_df)\n",
        "\n",
        "# Ensure the test set has the same columns as the train set\n",
        "train_columns = set(X_train_encoded.columns)\n",
        "test_columns = set(X_test_encoded.columns)\n",
        "\n",
        "missing_train_cols = test_columns - train_columns\n",
        "missing_test_cols = train_columns - test_columns\n",
        "\n",
        "for col in missing_train_cols:\n",
        "    X_train_encoded[col] = 0\n",
        "for col in missing_test_cols:\n",
        "    X_test_encoded[col] = 0\n",
        "\n",
        "# Reorder columns to match\n",
        "X_test_encoded = X_test_encoded[X_train_encoded.columns]\n",
        "\n",
        "# Standardize numerical features\n",
        "numerical_features = ['model_year', 'milage', 'engine_power', 'age']\n",
        "scaler = StandardScaler()\n",
        "X_train_encoded[numerical_features] = scaler.fit_transform(X_train_encoded[numerical_features])\n",
        "X_test_encoded[numerical_features] = scaler.transform(X_test_encoded[numerical_features])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Implement k-fold cross-validation for hyperparameter tuning\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "\n",
        "# Set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 5],\n",
        "    'subsample': [0.7, 0.8],\n",
        "    'colsample_bytree': [0.7, 0.8]\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=kf, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "val_predictions = best_xgb_model.predict(X_val)\n",
        "mse = mean_squared_error(y_val, val_predictions)\n",
        "r2 = r2_score(y_val, val_predictions)\n",
        "\n",
        "print(f'MSE: {mse}')\n",
        "print(f'R2 Score: {r2}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "test_predictions = best_xgb_model.predict(X_test_encoded)\n",
        "\n",
        "# Create a DataFrame for the results\n",
        "submission_df = pd.DataFrame({'id': test_df['id'], 'price': test_predictions.flatten()})\n",
        "\n",
        "# Save predictions to a CSV file in the required format\n",
        "submission_df.to_csv('Test_Predictions_Keras_Final_Best.csv', index=False)"
      ],
      "metadata": {
        "id": "pSZ1HxcYdDLk"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}